{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fd2596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f9c802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# llm=ChatOpenAI(openai_api_key=KEY, model_name=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "# load_dotenv()\n",
    "# KEY=os.getenv(\"OPEN_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac41ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline,AutoModelForCausalLM\n",
    "# from langchain_community.llms import HuggingFacePipeline\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "# import torch\n",
    "\n",
    "# # Explicitly load model + tokenizer\n",
    "# model_id = \"google/flan-t5-base\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "# # Now create the pipeline using loaded model + tokenizer\n",
    "# pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=100)\n",
    "\n",
    "# # Wrap with LangChain\n",
    "# llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# # Prompt using new syntax\n",
    "# prompt = PromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "# chain = prompt | llm\n",
    "\n",
    "# # Run it\n",
    "# result = chain.invoke({\"country\": \"India\"})\n",
    "# print(\"Answer:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25e3966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json = {\n",
    "    \"1\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"3\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8389041",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE1=\"\"\"\n",
    "Text:{text}\n",
    "You are an expert MCQ maker. Given the above text, it is your job to \\\n",
    "create a quiz  of {number} multiple choice questions for {subject} students in {tone} tone. \n",
    "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
    "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. \\\n",
    "Ensure to make {number} MCQs\n",
    "### RESPONSE_JSON\n",
    "{response_json}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eb7b72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Prathyusha\\mcqgen\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline,AutoModelForCausalLM\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import torch,json\n",
    "from langchain.chains import LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40393ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prathyusha\\AppData\\Local\\Temp\\ipykernel_2616\\3940201221.py:11: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n",
      "C:\\Users\\Prathyusha\\AppData\\Local\\Temp\\ipykernel_2616\\3940201221.py:13: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt1, output_key=\"quiz\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Explicitly load model + tokenizer\n",
    "model_id = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "# Now create the pipeline using loaded model + tokenizer\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=100)\n",
    "prompt1=PromptTemplate(input_variable=[\"text\",\"number\",\"subject\",\"tone\",\"response_json\"],\n",
    "                                      template=TEMPLATE1)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt1, output_key=\"quiz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab309909",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE2=\"\"\"\n",
    "You are an expert english grammarian and writer. Given a Multiple Choice Quiz for {subject} students.\\\n",
    "You need to evaluate the complexity of the question and give a complete analysis of the quiz. Only use at max 50 words for complexity analysis. \n",
    "if the quiz is not at per with the cognitive and analytical abilities of the students,\\\n",
    "update the quiz questions which needs to be changed and change the tone such that it perfectly fits the student abilities\n",
    "Quiz_MCQs:\n",
    "{quiz}\n",
    "\n",
    "Check from an expert English Writer of the above quiz:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f18ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "prompt2=PromptTemplate(input_variable=[\"subject\",\"quiz\"],\n",
    "                                      template=TEMPLATE2)\n",
    "\n",
    "\n",
    "llm2 = HuggingFacePipeline(pipeline=pipe)\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt2, output_key=\"review\")\n",
    "\n",
    "chain3 = SequentialChain(\n",
    "    chains=[chain, chain2],\n",
    "    input_variables=[\"subject\",\"text\", \"number\", \"tone\", \"response_json\"],\n",
    "    output_variables=[\"quiz\", \"review\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35a0c774",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=r\"C:/Users/Prathyusha/mcqgen/data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d63ab1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath, \"r\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d82980c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "output =chain.invoke({\n",
    "    \"subject\": \"English\",\n",
    "    \"text\": text,\n",
    "    \"number\": 3,\n",
    "    \"tone\": \"formal\",\n",
    "    \"response_json\": json.dumps(response_json, indent=4)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6691cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 'English',\n",
       " 'text': 'Data science is an interdisciplinary academic field[1] that uses statistics, scientific computing, scientific methods, processes, algorithms and systems to extract or extrapolate knowledge and insights from noisy, structured, and unstructured data.[2]\\n\\nData science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine).[3] Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.[4]\\n\\nData science is a \"concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data.[5] It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge.[6] However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.[7][8]\\n\\n',\n",
       " 'number': 3,\n",
       " 'tone': 'formal',\n",
       " 'response_json': '{\\n    \"1\": {\\n        \"mcq\": \"multiple choice question\",\\n        \"options\": {\\n            \"a\": \"choice here\",\\n            \"b\": \"choice here\",\\n            \"c\": \"choice here\",\\n            \"d\": \"choice here\"\\n        },\\n        \"correct\": \"correct answer\"\\n    },\\n    \"2\": {\\n        \"mcq\": \"multiple choice question\",\\n        \"options\": {\\n            \"a\": \"choice here\",\\n            \"b\": \"choice here\",\\n            \"c\": \"choice here\",\\n            \"d\": \"choice here\"\\n        },\\n        \"correct\": \"correct answer\"\\n    },\\n    \"3\": {\\n        \"mcq\": \"multiple choice question\",\\n        \"options\": {\\n            \"a\": \"choice here\",\\n            \"b\": \"choice here\",\\n            \"c\": \"choice here\",\\n            \"d\": \"choice here\"\\n        },\\n        \"correct\": \"correct answer\"\\n    }\\n}',\n",
       " 'quiz': ', , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d4e0db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-15-12-10-29_app.log\n",
      "c:\\Users\\Prathyusha\\mcqgen\\experiment\\logs\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "LOG_FILE=f\"{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}_app.log\"\n",
    "\n",
    "print(LOG_FILE)\n",
    "\n",
    "\n",
    "log_path=os.path.join(os.getcwd(), 'logs')\n",
    "print(log_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
